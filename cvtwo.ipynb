{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# OpenCV for Beginners\n",
        "\n",
        "*This Jupyter notebook is designed to introduce beginners to OpenCV, covering the basics with practical implementations and theoretical explanations.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## **1. Introduction to OpenCV**\n",
        "\n",
        "**OpenCV** (Open Source Computer Vision Library) is an open-source computer vision and machine learning library developed to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in commercial products. Initially developed by Intel, it supports various programming languages such as C++, Python, Java, and MATLAB, and is available on different platforms including Windows, Linux, macOS, iOS, and Android.\n",
        "\n",
        "### **Why OpenCV?**\n",
        "\n",
        "- **Wide Range of Functions**: OpenCV contains more than 2500 optimized algorithms for computer vision and machine learning.\n",
        "- **Real-Time Applications**: Designed for computational efficiency with a strong focus on real-time applications.\n",
        "- **Community and Support**: Large community of developers and comprehensive documentation.\n",
        "\n",
        "### **Applications of OpenCV**\n",
        "\n",
        "- **Image Processing**: Manipulation and transformation of images.\n",
        "- **Video Analysis**: Real-time video processing, object detection, and tracking.\n",
        "- **Machine Learning**: Implementing algorithms for facial recognition, object classification, etc.\n",
        "- **Augmented Reality**: Overlaying virtual content on real-world scenes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "## **2. Installing OpenCV in Google Colab**\n",
        "\n",
        "Google Colab is a cloud-based Jupyter notebook environment that allows you to write and execute Python code through your browser. It comes pre-installed with many libraries, but to ensure the latest version of OpenCV is available, we use `pip` to install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-opencv",
      "metadata": {
        "id": "install-opencv"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "## **3. Importing Libraries**\n",
        "\n",
        "We import the essential libraries required for image processing tasks:\n",
        "\n",
        "- `cv2`: The OpenCV library in Python.\n",
        "- `numpy`: A fundamental package for numerical computations, used for handling arrays.\n",
        "- `matplotlib`: A plotting library for creating static, animated, and interactive visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "import-libraries",
      "metadata": {
        "id": "import-libraries"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# For displaying videos\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "\n",
        "## **4. Reading and Displaying Images**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "#### **Reading Images**\n",
        "\n",
        "- **Function**: `cv2.imread(filename, flags)`\n",
        "- **Parameters**:\n",
        "  - `filename`: Name of the image file to be loaded.\n",
        "  - `flags`: Specifies the way in which the image should be read.\n",
        "    - `cv2.IMREAD_COLOR`: Loads a color image.\n",
        "    - `cv2.IMREAD_GRAYSCALE`: Loads image in grayscale mode.\n",
        "    - `cv2.IMREAD_UNCHANGED`: Loads image as is (including alpha channel).\n",
        "\n",
        "#### **Displaying Images**\n",
        "\n",
        "- In Jupyter notebooks, we use `matplotlib.pyplot.imshow()` to display images.\n",
        "- OpenCV uses BGR color space by default, while Matplotlib uses RGB. Therefore, we need to convert BGR images to RGB before displaying.\n",
        "\n",
        "#### **Color Spaces**\n",
        "\n",
        "- **BGR vs. RGB**: OpenCV uses BGR (Blue, Green, Red) while most libraries use RGB (Red, Green, Blue).\n",
        "\n",
        "### **Image Loading Process**\n",
        "\n",
        "1. **Upload**: Use Google Colab's `files.upload()` to upload images from your local machine.\n",
        "2. **Read**: Use `cv2.imread()` to load the image into a NumPy array.\n",
        "3. **Convert**: Change color space from BGR to RGB using `cv2.cvtColor()`.\n",
        "4. **Display**: Use `plt.imshow()` to display the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "read-display-image",
      "metadata": {
        "id": "read-display-image"
      },
      "outputs": [],
      "source": [
        "# Upload an image from your local machine\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Replace 'your_image.jpg' with the name of your uploaded image\n",
        "img = cv2.imread(list(uploaded.keys())[0])\n",
        "\n",
        "# Check if image was loaded\n",
        "if img is None:\n",
        "    print('Image not loaded. Please check the file name and path.')\n",
        "else:\n",
        "    # Convert BGR to RGB for display\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis('off')  # Hide axis\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "\n",
        "## **5. Basic Image Operations**\n",
        "\n",
        "Image processing involves manipulating images to achieve desired results. Basic operations include resizing, cropping, rotating, and translating images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "resize-image",
      "metadata": {
        "id": "resize-image"
      },
      "source": [
        "### **a. Resizing**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Resizing**: Changing the dimensions of an image.\n",
        "- **Function**: `cv2.resize(src, dsize, fx, fy, interpolation)`\n",
        "- **Parameters**:\n",
        "  - `src`: Input image.\n",
        "  - `dsize`: Desired size. If it is (0, 0), the size is calculated from `src` using `fx` and `fy`.\n",
        "  - `fx` and `fy`: Scale factors along the x and y axes.\n",
        "  - `interpolation`: Method used for resizing.\n",
        "    - Common methods: `cv2.INTER_LINEAR`, `cv2.INTER_AREA`, `cv2.INTER_CUBIC`.\n",
        "\n",
        "#### **Mathematical Model**\n",
        "\n",
        "Resizing an image involves mapping pixel coordinates from the original image to the new image based on scaling factors:\n",
        "\n",
        "\\[ x_{\\text{new}} = x_{\\text{old}} \\times \\text{fx} \\]\n",
        "\\[ y_{\\text{new}} = y_{\\text{old}} \\times \\text{fy} \\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-resize",
      "metadata": {
        "id": "code-resize"
      },
      "outputs": [],
      "source": [
        "# Resize the image to half its dimensions\n",
        "resized_img = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crop-image",
      "metadata": {
        "id": "crop-image"
      },
      "source": [
        "### **b. Cropping**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Cropping**: Extracting a region of interest (ROI) from an image.\n",
        "- Achieved by slicing the NumPy array representing the image.\n",
        "- Syntax: `cropped_img = img[startY:endY, startX:endX]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-crop",
      "metadata": {
        "id": "code-crop"
      },
      "outputs": [],
      "source": [
        "# Crop the image [startY:endY, startX:endX]\n",
        "cropped_img = img[100:400, 200:500]\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rotate-image",
      "metadata": {
        "id": "rotate-image"
      },
      "source": [
        "### **c. Rotating**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Rotation**: Rotating an image by a certain angle around a pivot point.\n",
        "- **Function**: `cv2.getRotationMatrix2D(center, angle, scale)`\n",
        "- **Transformation Matrix**:\n",
        "\n",
        "The rotation matrix \\( M \\) for rotating an image is defined as:\n",
        "\n",
        "$$ M = \\begin{bmatrix}\n",
        "\\alpha & \\beta & (1 - \\alpha) \\cdot center_x - \\beta \\cdot center_y \\\\\n",
        "-\\beta & \\alpha & \\beta \\cdot center_x + (1 - \\alpha) \\cdot center_y \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "$$ \\alpha = scale \\cdot \\cos(\\theta) $$  \n",
        "$$ \\beta = scale \\cdot \\sin(\\theta) $$\n",
        "\n",
        "\n",
        "\n",
        "- **Warping**: Apply the rotation matrix to the image using `cv2.warpAffine()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-rotate",
      "metadata": {
        "id": "code-rotate"
      },
      "outputs": [],
      "source": [
        "# Get image dimensions\n",
        "(h, w) = img.shape[:2]\n",
        "center = (w // 2, h // 2)\n",
        "\n",
        "# Rotate the image by 45 degrees\n",
        "M = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
        "rotated_img = cv2.warpAffine(img, M, (w, h))\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(rotated_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "## **6. Color Spaces**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Color Spaces**: Different ways to represent the colors in an image.\n",
        "- Common color spaces:\n",
        "  - **RGB/BGR**: Red, Green, Blue.\n",
        "  - **Grayscale**: Shades of gray, from black to white.\n",
        "  - **HSV**: Hue, Saturation, Value.\n",
        "  - **LAB**: Lightness and color-opponent dimensions.\n",
        "\n",
        "#### **Conversions**\n",
        "\n",
        "- Use `cv2.cvtColor()` to convert images between different color spaces.\n",
        "- **Example**: Convert BGR to Grayscale using `cv2.COLOR_BGR2GRAY`.\n",
        "\n",
        "### **Importance of Color Spaces**\n",
        "\n",
        "- Different color spaces are useful for different types of image processing tasks.\n",
        "- For example, HSV is useful for color-based segmentation because hue is independent of illumination.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "color-spaces",
      "metadata": {
        "id": "color-spaces"
      },
      "outputs": [],
      "source": [
        "# Convert BGR to Grayscale\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Display\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "## **7. Drawing Functions**\n",
        "\n",
        "OpenCV provides functions to draw shapes like lines, rectangles, circles, and text on images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "draw-line",
      "metadata": {
        "id": "draw-line"
      },
      "source": [
        "### **a. Drawing a Line**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Function**: `cv2.line(img, pt1, pt2, color, thickness)`\n",
        "- **Parameters**:\n",
        "  - `img`: Image on which to draw.\n",
        "  - `pt1`: Starting point (x1, y1).\n",
        "  - `pt2`: Ending point (x2, y2).\n",
        "  - `color`: Line color (B, G, R).\n",
        "  - `thickness`: Line thickness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-draw-line",
      "metadata": {
        "id": "code-draw-line"
      },
      "outputs": [],
      "source": [
        "# Copy the image to draw on\n",
        "line_img = img.copy()\n",
        "\n",
        "# Draw a line from top-left to bottom-right\n",
        "cv2.line(line_img, (0, 0), (line_img.shape[1], line_img.shape[0]), (255, 0, 0), 5)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(line_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "draw-rectangle",
      "metadata": {
        "id": "draw-rectangle"
      },
      "source": [
        "### **b. Drawing a Rectangle**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Function**: `cv2.rectangle(img, pt1, pt2, color, thickness)`\n",
        "- **Parameters**:\n",
        "  - `pt1`: One corner of the rectangle.\n",
        "  - `pt2`: Opposite corner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-draw-rectangle",
      "metadata": {
        "id": "code-draw-rectangle"
      },
      "outputs": [],
      "source": [
        "# Copy the image to draw on\n",
        "rect_img = img.copy()\n",
        "\n",
        "# Draw a rectangle\n",
        "cv2.rectangle(rect_img, (100, 100), (400, 300), (0, 255, 0), 3)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(rect_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "draw-circle",
      "metadata": {
        "id": "draw-circle"
      },
      "source": [
        "### **c. Drawing a Circle**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- **Function**: `cv2.circle(img, center, radius, color, thickness)`\n",
        "- **Parameters**:\n",
        "  - `center`: Center of the circle (x, y).\n",
        "  - `radius`: Radius of the circle.\n",
        "  - `thickness`: If `-1`, the circle is filled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-draw-circle",
      "metadata": {
        "id": "code-draw-circle"
      },
      "outputs": [],
      "source": [
        "# Copy the image to draw on\n",
        "circle_img = img.copy()\n",
        "\n",
        "# Draw a circle at the center\n",
        "center_coordinates = (circle_img.shape[1] // 2, circle_img.shape[0] // 2)\n",
        "cv2.circle(circle_img, center_coordinates, 100, (0, 0, 255), -1)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(circle_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "## **8. Image Arithmetic**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "#### **Image Addition**\n",
        "\n",
        "- Combining two images by adding the pixel values.\n",
        "- **Function**: `cv2.add(img1, img2)`\n",
        "- Pixel values are added up to a maximum of 255.\n",
        "\n",
        "#### **Blending**\n",
        "\n",
        "- Weighted addition of two images to create a blending effect.\n",
        "- **Function**: `cv2.addWeighted(img1, alpha, img2, beta, gamma)`\n",
        "- **Formula**:\n",
        "\n",
        "$$\n",
        "\\text{Result} = \\alpha \\times \\text{img1} + \\beta \\times \\text{img2} + \\gamma\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\alpha$ and $\\beta$ are weights.\n",
        "- $\\gamma$ is a scalar added to each sum.\n",
        "\n",
        "#### **Requirements**\n",
        "\n",
        "- Images must be of the same size and type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "image-arithmetic",
      "metadata": {
        "id": "image-arithmetic"
      },
      "outputs": [],
      "source": [
        "# Upload another image\n",
        "uploaded = files.upload()\n",
        "img2 = cv2.imread(list(uploaded.keys())[0])\n",
        "\n",
        "# Resize second image to match the first image\n",
        "img2 = cv2.resize(img2, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# Blend images\n",
        "blended_img = cv2.addWeighted(img, 0.7, img2, 0.3, 0)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(blended_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "## **9. Thresholding**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Thresholding**: Binarization of an image based on pixel intensity.\n",
        "- Converts grayscale images to binary images.\n",
        "- **Types of Thresholding**:\n",
        "  - **Global Thresholding**: Single threshold value for the entire image.\n",
        "  - **Adaptive Thresholding**: Threshold value calculated for smaller regions.\n",
        "  - **Otsu's Binarization**: Automatic threshold calculation.\n",
        "\n",
        "#### **Global Thresholding**\n",
        "\n",
        "- **Function**: `cv2.threshold(src, thresh, maxval, type)`\n",
        "- **Parameters**:\n",
        "  - `thresh`: Threshold value.\n",
        "  - `maxval`: Maximum value assigned to pixel values exceeding the threshold.\n",
        "  - `type`: Thresholding type (e.g., `cv2.THRESH_BINARY`).\n",
        "\n",
        "#### **Mathematical Model**\n",
        "\n",
        "$$\n",
        "\\text{dst}(x,y) = \\begin{cases}\n",
        "\\text{maxval} & \\text{if } \\text{src}(x,y) > \\text{thresh} \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thresholding",
      "metadata": {
        "id": "thresholding"
      },
      "outputs": [],
      "source": [
        "# Apply global thresholding\n",
        "ret, thresh_global = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Display\n",
        "plt.imshow(thresh_global, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-10",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "## **10. Blurring and Smoothing**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Purpose**: Reduce noise and details in images.\n",
        "- **Methods**:\n",
        "  - **Averaging (Mean Filter)**: Replaces each pixel value with the average of its neighbors.\n",
        "  - **Gaussian Blurring**: Weights pixels based on Gaussian distribution.\n",
        "  - **Median Blurring**: Replaces each pixel value with the median of its neighbors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blurring-average",
      "metadata": {
        "id": "blurring-average"
      },
      "source": [
        "#### **a. Averaging**\n",
        "\n",
        "- **Function**: `cv2.blur(src, ksize)`\n",
        "- **Kernel**: A matrix where all elements are equal and sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-blur-average",
      "metadata": {
        "id": "code-blur-average"
      },
      "outputs": [],
      "source": [
        "# Apply average blurring\n",
        "avg_blur = cv2.blur(img, (5, 5))\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(avg_blur, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blurring-gaussian",
      "metadata": {
        "id": "blurring-gaussian"
      },
      "source": [
        "#### **b. Gaussian Blur**\n",
        "\n",
        "- **Function**: `cv2.GaussianBlur(src, ksize, sigmaX)`\n",
        "- **Kernel**: Gaussian kernel calculated using the Gaussian function.\n",
        "\n",
        "#### **Gaussian Function**:\n",
        "\n",
        "$$\n",
        "G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-blur-gaussian",
      "metadata": {
        "id": "code-blur-gaussian"
      },
      "outputs": [],
      "source": [
        "# Apply Gaussian blurring\n",
        "gauss_blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(gauss_blur, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blurring-median",
      "metadata": {
        "id": "blurring-median"
      },
      "source": [
        "#### **c. Median Blur**\n",
        "\n",
        "- **Function**: `cv2.medianBlur(src, ksize)`\n",
        "- Effective for removing salt-and-pepper noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-blur-median",
      "metadata": {
        "id": "code-blur-median"
      },
      "outputs": [],
      "source": [
        "# Apply median blurring\n",
        "median_blur = cv2.medianBlur(img, 5)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(median_blur, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-11",
      "metadata": {
        "id": "section-11"
      },
      "source": [
        "## **11. Edge Detection**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Edges**: Significant transitions in pixel intensity.\n",
        "- **Purpose**: Identify object boundaries and features.\n",
        "\n",
        "#### **Canny Edge Detector**\n",
        "\n",
        "- **Function**: `cv2.Canny(image, threshold1, threshold2)`\n",
        "- **Steps**:\n",
        "  1. Noise reduction using Gaussian filter.\n",
        "  2. Finding intensity gradient of the image.\n",
        "  3. Non-maximum suppression to get rid of spurious response to edge detection.\n",
        "  4. Hysteresis thresholding to decide potential edges.\n",
        "\n",
        "#### **Mathematical Concepts**\n",
        "\n",
        "- **Gradient Calculation**: Using Sobel operators to find the gradient magnitude and direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edge-detection",
      "metadata": {
        "id": "edge-detection"
      },
      "outputs": [],
      "source": [
        "# Apply Canny Edge Detection\n",
        "edges = cv2.Canny(img, 100, 200)\n",
        "\n",
        "# Display\n",
        "plt.imshow(edges, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-12",
      "metadata": {
        "id": "section-12"
      },
      "source": [
        "## **12. Contours**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Contours**: Curves joining all continuous points along the boundary with the same color or intensity.\n",
        "- **Applications**: Shape analysis, object detection, recognition.\n",
        "\n",
        "#### **Finding Contours**\n",
        "\n",
        "- **Function**: `cv2.findContours(image, mode, method)`\n",
        "- **Parameters**:\n",
        "  - `mode`: Retrieval mode (e.g., `cv2.RETR_TREE`).\n",
        "  - `method`: Approximation method (e.g., `cv2.CHAIN_APPROX_SIMPLE`).\n",
        "\n",
        "#### **Drawing Contours**\n",
        "\n",
        "- **Function**: `cv2.drawContours(image, contours, contourIdx, color, thickness)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "contours",
      "metadata": {
        "id": "contours"
      },
      "outputs": [],
      "source": [
        "# Convert to grayscale and apply threshold\n",
        "ret, thresh = cv2.threshold(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), 127, 255, 0)\n",
        "\n",
        "# Find contours\n",
        "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw contours\n",
        "contour_img = img.copy()\n",
        "cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 3)\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-13",
      "metadata": {
        "id": "section-13"
      },
      "source": [
        "## **13. Histograms**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Histogram**: A graphical representation of the distribution of pixel intensities.\n",
        "- **Uses**:\n",
        "  - Analyze image contrast, brightness.\n",
        "  - Perform histogram equalization for contrast enhancement.\n",
        "\n",
        "#### **Calculating Histograms**\n",
        "\n",
        "- **Function**: `cv2.calcHist(images, channels, mask, histSize, ranges)`\n",
        "- **Parameters**:\n",
        "  - `images`: Source image.\n",
        "  - `channels`: Index of the channel (e.g., `[0]` for grayscale).\n",
        "  - `histSize`: Number of bins.\n",
        "  - `ranges`: Range of pixel values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "histograms",
      "metadata": {
        "id": "histograms"
      },
      "outputs": [],
      "source": [
        "# Calculate histogram\n",
        "hist = cv2.calcHist([gray_img], [0], None, [256], [0, 256])\n",
        "\n",
        "# Plot histogram\n",
        "plt.plot(hist)\n",
        "plt.title('Grayscale Histogram')\n",
        "plt.xlabel('Pixel Intensity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-14",
      "metadata": {
        "id": "section-14"
      },
      "source": [
        "---\n",
        "\n",
        "## **14. Morphological Operations**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Purpose**: Remove imperfections introduced during thresholding.\n",
        "- **Types**:\n",
        "  - **Erosion**: Removes pixels on object boundaries.\n",
        "  - **Dilation**: Adds pixels to object boundaries.\n",
        "  - **Opening**: Erosion followed by dilation.\n",
        "  - **Closing**: Dilation followed by erosion.\n",
        "\n",
        "#### **Structuring Element (Kernel)**\n",
        "\n",
        "- Defines the neighborhood over which the operation is applied.\n",
        "- Commonly a square or circular matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "morphology-erosion",
      "metadata": {
        "id": "morphology-erosion"
      },
      "source": [
        "#### **a. Erosion**\n",
        "\n",
        "- **Function**: `cv2.erode(src, kernel, iterations)`\n",
        "- **Effect**: Shrinks bright regions.\n",
        "\n",
        "#### **Mathematical Model**\n",
        "\n",
        "$$\n",
        "\\text{Erosion}: A \\ominus B = \\{ z | (B)_z \\subseteq A \\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "erosion",
      "metadata": {
        "id": "erosion"
      },
      "outputs": [],
      "source": [
        "# Define kernel\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "\n",
        "# Apply erosion\n",
        "erosion = cv2.erode(thresh_global, kernel, iterations=1)\n",
        "\n",
        "# Display\n",
        "plt.imshow(erosion, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "morphology-dilation",
      "metadata": {
        "id": "morphology-dilation"
      },
      "source": [
        "#### **b. Dilation**\n",
        "\n",
        "- **Function**: `cv2.dilate(src, kernel, iterations)`\n",
        "- **Effect**: Expands bright regions.\n",
        "\n",
        "#### **Mathematical Model**\n",
        "\n",
        "$$\n",
        "\\text{Dilation}: A \\oplus B = \\{ z | (B̂)_z \\cap A \\neq \\emptyset \\}\n",
        "$$\n",
        "\n",
        "Where $ A $ is the image and $ B $ is the structuring element.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dilation",
      "metadata": {
        "id": "dilation"
      },
      "outputs": [],
      "source": [
        "# Apply dilation\n",
        "dilation = cv2.dilate(thresh_global, kernel, iterations=1)\n",
        "\n",
        "# Display\n",
        "plt.imshow(dilation, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-15",
      "metadata": {
        "id": "section-15"
      },
      "source": [
        "## **15. Image Transformations**\n",
        "\n",
        "Image transformations involve changing the geometric configuration of an image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "affine-transformation",
      "metadata": {
        "id": "affine-transformation"
      },
      "source": [
        "### **a. Affine Transformation**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- Preserves lines and parallelism (not necessarily distances and angles).\n",
        "- **Transformation Matrix**:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x' \\\\\n",
        "y' \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "a_{00} & a_{01} \\\\\n",
        "a_{10} & a_{11} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "b_0 \\\\\n",
        "b_1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- **Function**: `cv2.getAffineTransform(pts1, pts2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affine-transformation-code",
      "metadata": {
        "id": "affine-transformation-code"
      },
      "outputs": [],
      "source": [
        "# Define points for transformation\n",
        "rows, cols = img.shape[:2]\n",
        "pts1 = np.float32([[50,50], [200,50], [50,200]])\n",
        "pts2 = np.float32([[10,100], [200,50], [100,250]])\n",
        "\n",
        "# Get the transformation matrix\n",
        "M = cv2.getAffineTransform(pts1, pts2)\n",
        "\n",
        "# Apply affine transformation\n",
        "affine_img = cv2.warpAffine(img, M, (cols, rows))\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(affine_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perspective-transformation",
      "metadata": {
        "id": "perspective-transformation"
      },
      "source": [
        "### **b. Perspective Transformation**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- Straight lines remain straight, but parallel lines may not remain parallel.\n",
        "- **Transformation Matrix**:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x' \\\\\n",
        "y' \\\\\n",
        "w \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "p_{00} & p_{01} & p_{02} \\\\\n",
        "p_{10} & p_{11} & p_{12} \\\\\n",
        "p_{20} & p_{21} & p_{22} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then, $ x_{\\text{new}} = x'/w, y_{\\text{new}} = y'/w $.\n",
        "\n",
        "- **Function**: `cv2.getPerspectiveTransform(pts1, pts2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "perspective-transformation-code",
      "metadata": {
        "id": "perspective-transformation-code"
      },
      "outputs": [],
      "source": [
        "# Define points for transformation\n",
        "pts1 = np.float32([[56,65], [368,52], [28,387], [389,390]])\n",
        "pts2 = np.float32([[0,0], [300,0], [0,300], [300,300]])\n",
        "\n",
        "# Get the transformation matrix\n",
        "M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "\n",
        "# Apply perspective transformation\n",
        "perspective_img = cv2.warpPerspective(img, M, (300, 300))\n",
        "\n",
        "# Display\n",
        "plt.imshow(cv2.cvtColor(perspective_img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-16",
      "metadata": {
        "id": "section-16"
      },
      "source": [
        "---\n",
        "\n",
        "## **16. Video Processing**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- Processing video frames in real-time.\n",
        "- Capturing video involves reading frames in a loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video-capture",
      "metadata": {
        "id": "video-capture"
      },
      "source": [
        "### **a. Capturing Video from Webcam**\n",
        "\n",
        "#### **Note**\n",
        "\n",
        "- Not supported in Google Colab due to lack of access to local hardware.\n",
        "\n",
        "#### **Function**\n",
        "\n",
        "- **Initialization**: `cv2.VideoCapture(0)`\n",
        "- **Reading Frames**: `cap.read()`\n",
        "- **Displaying Frames**: `cv2.imshow()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video-capture-code",
      "metadata": {
        "id": "video-capture-code"
      },
      "outputs": [],
      "source": [
        "# This code may not work in Google Colab due to lack of webcam access\n",
        "\n",
        "# Initialize webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('Webcam', frame)\n",
        "\n",
        "    # Press 'q' to exit\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the capture\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video-processing",
      "metadata": {
        "id": "video-processing"
      },
      "source": [
        "### **b. Processing Video File**\n",
        "\n",
        "#### **Theory**\n",
        "\n",
        "- Similar to image processing, but applied to each frame.\n",
        "- Useful for tasks like video stabilization, object tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video-file-processing",
      "metadata": {
        "id": "video-file-processing"
      },
      "outputs": [],
      "source": [
        "# Upload a video file\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Display frame\n",
        "    plt.imshow(gray, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Clear output to display next frame\n",
        "    clear_output(wait=True)\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-17",
      "metadata": {
        "id": "section-17"
      },
      "source": [
        "## **17. Face Detection with Haar Cascades**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Haar Cascades**: Machine learning-based approach where a cascade function is trained from a lot of positive and negative images.\n",
        "- **Features**: Uses Haar-like features which are digital image features used in object recognition.\n",
        "\n",
        "#### **Algorithm Steps**\n",
        "\n",
        "1. **Haar Feature Selection**: Identifies features such as edges and lines.\n",
        "2. **Integral Images**: Quickly computes the sum of values in a rectangular subset of a grid.\n",
        "3. **Adaboost Training**: Selects the best features and trains classifiers.\n",
        "4. **Cascading Classifiers**: Combines many weak classifiers to form a strong classifier.\n",
        "\n",
        "#### **Function**\n",
        "\n",
        "- **Loading Classifier**: `cv2.CascadeClassifier()`\n",
        "- **Detection**: `detectMultiScale()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "face-detection",
      "metadata": {
        "id": "face-detection"
      },
      "outputs": [],
      "source": [
        "# Load the cascade\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "# Draw rectangle around the faces\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "# Display the output\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-18",
      "metadata": {
        "id": "section-18"
      },
      "source": [
        "## **18. Feature Detection using ORB**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **ORB (Oriented FAST and Rotated BRIEF)**: Efficient alternative to SIFT and SURF.\n",
        "- **Features**:\n",
        "  - **FAST**: Features from Accelerated Segment Test for keypoint detection.\n",
        "  - **BRIEF**: Binary Robust Independent Elementary Features for descriptor extraction.\n",
        "\n",
        "#### **Steps**\n",
        "\n",
        "1. **Keypoint Detection**: Identifies points of interest in the image.\n",
        "2. **Descriptor Computation**: Describes the neighborhood of keypoints.\n",
        "3. **Feature Matching**: Matching descriptors between images.\n",
        "\n",
        "#### **Advantages**\n",
        "\n",
        "- Fast and computationally efficient.\n",
        "- Scale and rotation invariant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-detection",
      "metadata": {
        "id": "feature-detection"
      },
      "outputs": [],
      "source": [
        "# Initialize ORB detector\n",
        "orb = cv2.ORB_create()\n",
        "\n",
        "# Find the keypoints with ORB\n",
        "kp = orb.detect(img, None)\n",
        "\n",
        "# Compute the descriptors\n",
        "kp, des = orb.compute(img, kp)\n",
        "\n",
        "# Draw only keypoints location (not size and orientation)\n",
        "img_with_kp = cv2.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(cv2.cvtColor(img_with_kp, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-19",
      "metadata": {
        "id": "section-19"
      },
      "source": [
        "## **19. Object Tracking with Camshift**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Camshift Algorithm**: Continuously Adaptive Mean Shift algorithm for object tracking.\n",
        "- **Uses**: Tracking objects whose appearance may change over time.\n",
        "\n",
        "#### **Algorithm Steps**\n",
        "\n",
        "1. **Select ROI**: Define the initial position of the object.\n",
        "2. **Compute Histogram**: Calculate the color histogram of the ROI.\n",
        "3. **Backprojection**: Project the histogram back onto the image to find the object.\n",
        "4. **Mean Shift**: Iteratively move the search window to the area of highest density.\n",
        "5. **Update**: Adjust the size and orientation of the window.\n",
        "\n",
        "#### **Mathematical Concepts**\n",
        "\n",
        "- **Mean Shift Vector**:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum_{i} x_i \\cdot w(x_i)}{\\sum_{i} w(x_i)}\n",
        "$$\n",
        "\n",
        "Where $ w(x_i) $ is the weight of pixel $ x_i $.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "object-tracking",
      "metadata": {
        "id": "object-tracking"
      },
      "outputs": [],
      "source": [
        "# This is a simplified example and may not work in Colab due to video capture limitations\n",
        "\n",
        "# Set up initial location of window\n",
        "r, h, c, w = 250, 90, 400, 125  # Hardcoded values\n",
        "track_window = (c, r, w, h)\n",
        "\n",
        "# Read the first frame of the video\n",
        "ret, frame = cap.read()\n",
        "\n",
        "# Set up the ROI for tracking\n",
        "roi = frame[r:r+h, c:c+w]\n",
        "hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
        "mask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n",
        "roi_hist = cv2.calcHist([hsv_roi],[0], mask,[180],[0,180])\n",
        "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
        "\n",
        "# Setup the termination criteria\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "\n",
        "while True:\n",
        "    ret ,frame = cap.read()\n",
        "    if ret == True:\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
        "\n",
        "        # Apply CamShift to get the new location\n",
        "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
        "\n",
        "        # Draw it on image\n",
        "        pts = cv2.boxPoints(ret)\n",
        "        pts = np.int0(pts)\n",
        "        img2 = cv2.polylines(frame,[pts],True, 255,2)\n",
        "\n",
        "        # Display\n",
        "        plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Clear output to display next frame\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-20",
      "metadata": {
        "id": "section-20"
      },
      "source": [
        "\n",
        "## **20. Template Matching**\n",
        "\n",
        "### **Theory**\n",
        "\n",
        "- **Template Matching**: Finding the occurrence of a template image within a larger image.\n",
        "- **Methods**:\n",
        "  - **Correlation-Based Matching**: Measures similarity between template and image regions.\n",
        "  - **Matching Methods**: `cv2.TM_CCOEFF`, `cv2.TM_SQDIFF`, etc.\n",
        "\n",
        "#### **Function**\n",
        "\n",
        "- **Matching**: `cv2.matchTemplate(image, template, method)`\n",
        "- **Normalization**: Result is normalized between 0 and 1.\n",
        "- **Thresholding**: Define a threshold to identify matches.\n",
        "\n",
        "#### **Mathematical Model**\n",
        "\n",
        "- **Cross-Correlation**:\n",
        "\n",
        "$$\n",
        "R(x, y) = \\sum_{i,j} \\left[ T(i,j) \\cdot I(x+i, y+j) \\right]\n",
        "$$\n",
        "\n",
        "Where $ T $ is the template, and $ I $ is the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "template-matching",
      "metadata": {
        "id": "template-matching"
      },
      "outputs": [],
      "source": [
        "# Upload template image\n",
        "uploaded = files.upload()\n",
        "template = cv2.imread(list(uploaded.keys())[0], 0)\n",
        "w, h = template.shape[::-1]\n",
        "\n",
        "# Convert main image to grayscale\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply template Matching\n",
        "res = cv2.matchTemplate(img_gray, template, cv2.TM_CCOEFF_NORMED)\n",
        "threshold = 0.8\n",
        "loc = np.where(res >= threshold)\n",
        "\n",
        "for pt in zip(*loc[::-1]):\n",
        "    cv2.rectangle(img, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n",
        "\n",
        "# Display the result\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "This notebook provided an extensive introduction to OpenCV, covering fundamental concepts, practical implementations, and theoretical underpinnings of various image processing techniques. Understanding both the code and the mathematical models behind these operations is crucial for developing advanced computer vision applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}